#cloud-config

# This script should be supplied as 'user-data' for a GCE VM instance that runs the Container-Optimized OS
# The instance is expected to have an additional disk attached
# The disk will be used for persistent storage -- caching agent logs, and build job workspace between runs
#   and VM restarts (thereby allowing for incremental builds)
#
# The VM will be idle, and wait for the Jenkins controller to log in via SSH as 'jenkins-ssh' and run with '/var/jenkins-agent-wrapper.sh' as the java executable path.
# That script, in turn, will wait for configuration to be present in GCP's Secret Manager, and then bring up a Jenkins
#   agent within a Docker container
# The build jobs can either run within the agent's container, or use Jenkins' Docker plugin to run jobs
#   within other containers (thereby allowing for 'build tools containers')

# Debugging tips:
# - View /var/log/cloud-init-output.log to see stdout/stderr info from the various step
# - View /var/log/cloud-init.log to see cloud-init's internal logs

# Module execution order:
# - bootcmd
# - users
# - write_files
# - runcmd

# Reference: https://cloudinit.readthedocs.io/en/latest/topics/modules.html#bootcmd
bootcmd:
  # Stop SSH daemon
  # This blocks the Jenkins controller from accessing this instance until initialization is complete.
  # Well -- there is probably still a time window where sshd does not stop as it should;
  #   this results in occasional startup failures for instances.
  - sudo systemctl stop sshd

  # Mount persistent disk; if mount fails, then create a filesystem on the persistent disk, and retry
  - >-
    mkdir -p /mnt/disks/persistent-disk
    && if ! mount -o discard,defaults,rw /dev/sdb /mnt/disks/persistent-disk; then
    mkfs -t ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb;
    mount -o discard,defaults,rw /dev/sdb /mnt/disks/persistent-disk;
    fi
    && chmod ugo+rwx /mnt/disks/persistent-disk

  # Create agent folder
  - mkdir /mnt/disks/persistent-disk/agent && chmod ugo+rwx /mnt/disks/persistent-disk/agent

  # Create workspace folder
  - mkdir /mnt/disks/persistent-disk/workspace && chmod ugo+rwx /mnt/disks/persistent-disk/workspace

  # Create Plastic SCM config folder
  - mkdir /mnt/disks/persistent-disk/.plastic4 && chmod ugo+rwx /mnt/disks/persistent-disk/.plastic4

  # Allow everyone to access this Docker daemon socket
  # It is only the 'jenkins' (uid=1000) that needs access, but the socket's user/group shows like this from within the container:
  #   jenkins@194572782dea:~$ ls -l /var/run/docker.sock
  #   srw-rw---- 1 nobody nogroup 0 May  2 17:29 /var/run/docker.sock
  # The most straightforward way to make this work is to grant everyone access to the socket
  # Alternatively, we could add 'chronos' to the docker group
  - chmod ugo+rw /var/run/docker.sock

users:
  - name: jenkins-ssh
    gecos: Jenkins SSH
    primary_group: jenkins-ssh
    groups: wheel

# Reference: https://cloudinit.readthedocs.io/en/latest/topics/modules.html#write-files
write_files:
  - path: /run/jenkins-agent-wrapper.sh
    permissions: 0744
    owner: jenkins-ssh
    content: |
      #!/bin/bash

      # This script is expected to be launched either like this:
      #   /run/jenkins-agent-wrapper.sh -fullversion
      # or like this:
      #   /run/jenkins-agent-wrapper.sh -jar <path to agent.jar>

      # Respond to version query; the GCE plugin does this to verify that the java executable is present, and doesn't care about the actual version number
      if [[ "$1" == "-fullversion" ]]; then
        echo "java-to-docker shim"
        exit 0
      fi

      # If it isn't a version query, then we require it to be a launch command
      if [[ $# -ne 2 || "$1" != "-jar" ]]; then
        echo "Error: java-to-docker shim should be executed like this: /run/jenkins-agent-wrapper.sh -jar <path to agent.jar>"
        exit 1
      fi

      JAR_FILE=$2

      # Wait for all cloud-init scripts to finish
      cloud-init status --wait 2>&1

      # Ensure agent.jar is readable by the 'chronos' user
      chmod ugo+r ${JAR_FILE}

      # Run the inner script in the context of the 'chronos' user. This user has uid=1000, which happens to match the
      #   ID of the Jenkins user within the Docker container that we will launch.
      sudo -u chronos -i /run/jenkins-agent.sh ${JAR_FILE}

  - path: /run/jenkins-agent.sh
    permissions: 0744
    # The user 'chronos' has uid=1000 within the host VM, and this corresponds to the 'jenkins' user within the contaienr
    owner: chronos
    content: |
      #!/bin/bash

      JAR_FILE=$1

      # Retrieve hostname from GCE instance metadata
      get_gce_instance_hostname () {

        local METADATA=http://metadata.google.internal/computeMetadata/v1
        local HOSTNAME=$(curl -H 'Metadata-Flavor: Google' "${METADATA}/instance/hostname")

        echo "${HOSTNAME}"
      }

      # Retrieve a secret from GCP Secret Manager (base64-encoded)
      # If the secret is not known, returns ""
      get_gce_secret_base64 () {

        local SECRET_NAME=$1

        local METADATA=http://metadata.google.internal/computeMetadata/v1

        # Fetch access token for google APIs
        local ACCESS_TOKEN=$(curl -H 'Metadata-Flavor: Google' "${METADATA}/instance/service-accounts/default/token" | cut -d '"' -f 4)
        #Typical response format from curl call:
        #   {
        #     "name": "projects/<id>/secrets/<name>/versions/1",
        #     "payload": {
        #       "data": "<base64 encoded string>"
        #     }
        #   }

        local PROJECT=$(curl -H 'Metadata-Flavor: Google' ${METADATA}/project/project-id)

        local SECRET_RESPONSE=$(curl https://secretmanager.googleapis.com/v1/projects/${PROJECT}/secrets/${SECRET_NAME}/versions/latest:access -H "Authorization: Bearer ${ACCESS_TOKEN}")

        if [ $(echo ${SECRET_RESPONSE} | awk -F "\"" '{print $2}') != 'error' ]; then
          local SECRET=$(echo ${SECRET_RESPONSE} | awk -F "\"" '{print $10}')
          echo $SECRET
        fi
      }

      AGENT_NAME=$(get_gce_instance_hostname | cut -d '.' -f 1)

      echo "Agent name: ${AGENT_NAME}"

      # Wait for necessary parameters to become available in GCE's Secret Manager

      while :
      do
        echo "Retrieving configuration from Secrets Manager..."
        ACCESS_KEY="$(get_gce_secret_base64 "agent-key-file" | base64 --decode)"
        AGENT_IMAGE_URL_LINUX="$(get_gce_secret_base64 "ssh-agent-image-url-linux" | base64 --decode)"
        PLASTIC_CONFIG_BASE64="$(get_gce_secret_base64 "plastic-config-tgz")"

        echo "Required secrets/instance metadata:"
        echo "Secret agent-key-file: $([[ "${ACCESS_KEY}" != "" ]] && echo "found" || echo "not found")"
        echo "Secret ssh-agent-image-url-linux: $([[ "${AGENT_IMAGE_URL_LINUX}" != "" ]] && echo "found" || echo "not found")"
        echo "Optional secrets:"
        echo "Secret plastic-config-tgz: $([[ "${PLASTIC_CONFIG_BASE64}" != "" ]] && echo "found" || echo "not found")"

        if [ "${ACCESS_KEY}" != "" ] && [ "${AGENT_IMAGE_URL_LINUX}" != "" ]; then
          break
        fi

        echo "Some required secrets are missing. Sleeping, then retrying..."
        sleep 10
      done

      if [ "${PLASTIC_CONFIG_BASE64}" != "" ]; then
        echo "Deploying Plastic SCM client configuration..."
        echo "${PLASTIC_CONFIG_BASE64}" | base64 --decode | tar -zxv --directory=/mnt/disks/persistent-disk/.plastic4
      fi

      echo "Authenticating Docker for Google Artifact Registry..."

      # Use JSON key-based authentication for Docker
      #
      # While we could rely on the application default credentials (using the instance's own service account) instead,
      #   that would work on the host VM but not within the jenkins-agent container (the instance metadata service
      #   is not available at 169.254.169.254 there). The simplest way for us to get Docker authentication to work
      #   both on the host VM and within the container is to switch to JSON key authentication and then
      #   mount the docker config file into the jenkins-agent container.


      LOCATION=$(echo "${AGENT_IMAGE_URL_LINUX}" | cut -d '-' -f 1-2)
      # AGENT_IMAGE_URL_LINUX typically looks like: europe-west1-docker.pkg.dev/<project>/<folder>/<file>
      echo "${ACCESS_KEY}" | docker login -u _json_key --password-stdin "https://${LOCATION}-docker.pkg.dev" 2>&1

      if [[ ! $? ]]; then
        echo "docker login exited with code $?"
      fi

      echo "Starting Jenkins Agent container..."

      # Launch jenkins-agent container
      #
      # The mounts serve different purposes:
      # - /usr/share/jenkins/controller-provided-agent.jar - allows us to run the agent jar that the Jenkins controller supplied
      # - /home/jenkins/.docker/config.json - allows pulling images from Google Artifact Registry from within the container
      # - /home/jenkins/.plastic4 - allows using Plastic SCM tools from within the container
      # - /var/run/docker.sock - allows for interacting with the Docker daemon from within the container
      #   (This has the same path on the host as within the container, since the 'docker' commandline tool requires it)
      # - /mnt/disks/persistent-disk/agent - allows for storing logs and .JAR cache somewhere that persists between host VM restarts
      # - /mnt/disks/persistent-disk/workspace - allows for storing build job workspace somewhere that persists between host VM restarts
      #   (This has the same path on the host as within the container, since the "copy over agent.jar from controller to node" logic will copy it into this folder on the host)
      #
      # All these mounts will also be used in any additional containers started by the build job,
      #   so all these resources will be accessible by the build job's own script logic;
      #   in theory, the build job only needs access to /mnt/disks/persistent-disk/workspace but Jenkins offers no means
      #   for restricting which of these mounts will be present when the build job's logic runs

      docker \
        run \
        --rm \
        --init \
        -i \
        --name=jenkins-agent \
        --mount type=bind,source=${JAR_FILE},destination=/usr/share/jenkins/controller-provided-agent.jar \
        --mount type=bind,source=/home/chronos/user/.docker/config.json,destination=/home/jenkins/.docker/config.json \
        --mount type=bind,source=/mnt/disks/persistent-disk/.plastic4,destination=/home/jenkins/.plastic4 \
        --mount type=bind,source=/mnt/disks/persistent-disk/agent,destination=/mnt/disks/persistent-disk/agent \
        --mount type=bind,source=/mnt/disks/persistent-disk/workspace,destination=/mnt/disks/persistent-disk/workspace \
        -v /var/run/docker.sock:/var/run/docker.sock \
        "${AGENT_IMAGE_URL_LINUX}" \
        java -jar /usr/share/jenkins/controller-provided-agent.jar \
        -text \
        -workDir /mnt/disks/persistent-disk/agent

      if [[ ! $? ]]; then
        echo "docker run exited with code $?"
      fi

      echo "Jenkins Agent container has stopped."

# Reference: https://cloudinit.readthedocs.io/en/latest/topics/modules.html#runcmd
runcmd:
  # Start SSH Daemon
  # The Jenkins controller is now free to initiate contact with this instance
  - sudo systemctl start sshd
